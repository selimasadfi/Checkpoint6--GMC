{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e6f6c4",
   "metadata": {},
   "source": [
    "Checkpoint Objective\n",
    "\n",
    "Implement KNN from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52b949",
   "metadata": {},
   "source": [
    "The test problem we will be using in this tutorial is the iris classification.\n",
    "\n",
    "The problem consists of 150 observations of iris flowers from three different species. There are 4 measurements of given flowers: sepal length, sepal width, petal length, and petal width (all in the same unit of centimeters). The predicted attribute is the species, which is either Setosa, Versicolor, or Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f541d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import operator #operator module for sorting\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065bda91",
   "metadata": {},
   "source": [
    "1. Handle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b0d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1, 3.5, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.0, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.3, 0.2, Iris-setosa\n",
      "4.6, 3.1, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.6, 1.4, 0.2, Iris-setosa\n",
      "5.4, 3.9, 1.7, 0.4, Iris-setosa\n",
      "4.6, 3.4, 1.4, 0.3, Iris-setosa\n",
      "5.0, 3.4, 1.5, 0.2, Iris-setosa\n",
      "4.4, 2.9, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.4, 3.7, 1.5, 0.2, Iris-setosa\n",
      "4.8, 3.4, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.1, Iris-setosa\n",
      "4.3, 3.0, 1.1, 0.1, Iris-setosa\n",
      "5.8, 4.0, 1.2, 0.2, Iris-setosa\n",
      "5.7, 4.4, 1.5, 0.4, Iris-setosa\n",
      "5.4, 3.9, 1.3, 0.4, Iris-setosa\n",
      "5.1, 3.5, 1.4, 0.3, Iris-setosa\n",
      "5.7, 3.8, 1.7, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.5, 0.3, Iris-setosa\n",
      "5.4, 3.4, 1.7, 0.2, Iris-setosa\n",
      "5.1, 3.7, 1.5, 0.4, Iris-setosa\n",
      "4.6, 3.6, 1.0, 0.2, Iris-setosa\n",
      "5.1, 3.3, 1.7, 0.5, Iris-setosa\n",
      "4.8, 3.4, 1.9, 0.2, Iris-setosa\n",
      "5.0, 3.0, 1.6, 0.2, Iris-setosa\n",
      "5.0, 3.4, 1.6, 0.4, Iris-setosa\n",
      "5.2, 3.5, 1.5, 0.2, Iris-setosa\n",
      "5.2, 3.4, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.1, 1.6, 0.2, Iris-setosa\n",
      "5.4, 3.4, 1.5, 0.4, Iris-setosa\n",
      "5.2, 4.1, 1.5, 0.1, Iris-setosa\n",
      "5.5, 4.2, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.0, 3.2, 1.2, 0.2, Iris-setosa\n",
      "5.5, 3.5, 1.3, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "4.4, 3.0, 1.3, 0.2, Iris-setosa\n",
      "5.1, 3.4, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.3, 0.3, Iris-setosa\n",
      "4.5, 2.3, 1.3, 0.3, Iris-setosa\n",
      "4.4, 3.2, 1.3, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.6, 0.6, Iris-setosa\n",
      "5.1, 3.8, 1.9, 0.4, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.6, 0.2, Iris-setosa\n",
      "4.6, 3.2, 1.4, 0.2, Iris-setosa\n",
      "5.3, 3.7, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.3, 1.4, 0.2, Iris-setosa\n",
      "7.0, 3.2, 4.7, 1.4, Iris-versicolor\n",
      "6.4, 3.2, 4.5, 1.5, Iris-versicolor\n",
      "6.9, 3.1, 4.9, 1.5, Iris-versicolor\n",
      "5.5, 2.3, 4.0, 1.3, Iris-versicolor\n",
      "6.5, 2.8, 4.6, 1.5, Iris-versicolor\n",
      "5.7, 2.8, 4.5, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 4.7, 1.6, Iris-versicolor\n",
      "4.9, 2.4, 3.3, 1.0, Iris-versicolor\n",
      "6.6, 2.9, 4.6, 1.3, Iris-versicolor\n",
      "5.2, 2.7, 3.9, 1.4, Iris-versicolor\n",
      "5.0, 2.0, 3.5, 1.0, Iris-versicolor\n",
      "5.9, 3.0, 4.2, 1.5, Iris-versicolor\n",
      "6.0, 2.2, 4.0, 1.0, Iris-versicolor\n",
      "6.1, 2.9, 4.7, 1.4, Iris-versicolor\n",
      "5.6, 2.9, 3.6, 1.3, Iris-versicolor\n",
      "6.7, 3.1, 4.4, 1.4, Iris-versicolor\n",
      "5.6, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "5.8, 2.7, 4.1, 1.0, Iris-versicolor\n",
      "6.2, 2.2, 4.5, 1.5, Iris-versicolor\n",
      "5.6, 2.5, 3.9, 1.1, Iris-versicolor\n",
      "5.9, 3.2, 4.8, 1.8, Iris-versicolor\n",
      "6.1, 2.8, 4.0, 1.3, Iris-versicolor\n",
      "6.3, 2.5, 4.9, 1.5, Iris-versicolor\n",
      "6.1, 2.8, 4.7, 1.2, Iris-versicolor\n",
      "6.4, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "6.6, 3.0, 4.4, 1.4, Iris-versicolor\n",
      "6.8, 2.8, 4.8, 1.4, Iris-versicolor\n",
      "6.7, 3.0, 5.0, 1.7, Iris-versicolor\n",
      "6.0, 2.9, 4.5, 1.5, Iris-versicolor\n",
      "5.7, 2.6, 3.5, 1.0, Iris-versicolor\n",
      "5.5, 2.4, 3.8, 1.1, Iris-versicolor\n",
      "5.5, 2.4, 3.7, 1.0, Iris-versicolor\n",
      "5.8, 2.7, 3.9, 1.2, Iris-versicolor\n",
      "6.0, 2.7, 5.1, 1.6, Iris-versicolor\n",
      "5.4, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "6.0, 3.4, 4.5, 1.6, Iris-versicolor\n",
      "6.7, 3.1, 4.7, 1.5, Iris-versicolor\n",
      "6.3, 2.3, 4.4, 1.3, Iris-versicolor\n",
      "5.6, 3.0, 4.1, 1.3, Iris-versicolor\n",
      "5.5, 2.5, 4.0, 1.3, Iris-versicolor\n",
      "5.5, 2.6, 4.4, 1.2, Iris-versicolor\n",
      "6.1, 3.0, 4.6, 1.4, Iris-versicolor\n",
      "5.8, 2.6, 4.0, 1.2, Iris-versicolor\n",
      "5.0, 2.3, 3.3, 1.0, Iris-versicolor\n",
      "5.6, 2.7, 4.2, 1.3, Iris-versicolor\n",
      "5.7, 3.0, 4.2, 1.2, Iris-versicolor\n",
      "5.7, 2.9, 4.2, 1.3, Iris-versicolor\n",
      "6.2, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "5.1, 2.5, 3.0, 1.1, Iris-versicolor\n",
      "5.7, 2.8, 4.1, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 6.0, 2.5, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "7.1, 3.0, 5.9, 2.1, Iris-virginica\n",
      "6.3, 2.9, 5.6, 1.8, Iris-virginica\n",
      "6.5, 3.0, 5.8, 2.2, Iris-virginica\n",
      "7.6, 3.0, 6.6, 2.1, Iris-virginica\n",
      "4.9, 2.5, 4.5, 1.7, Iris-virginica\n",
      "7.3, 2.9, 6.3, 1.8, Iris-virginica\n",
      "6.7, 2.5, 5.8, 1.8, Iris-virginica\n",
      "7.2, 3.6, 6.1, 2.5, Iris-virginica\n",
      "6.5, 3.2, 5.1, 2.0, Iris-virginica\n",
      "6.4, 2.7, 5.3, 1.9, Iris-virginica\n",
      "6.8, 3.0, 5.5, 2.1, Iris-virginica\n",
      "5.7, 2.5, 5.0, 2.0, Iris-virginica\n",
      "5.8, 2.8, 5.1, 2.4, Iris-virginica\n",
      "6.4, 3.2, 5.3, 2.3, Iris-virginica\n",
      "6.5, 3.0, 5.5, 1.8, Iris-virginica\n",
      "7.7, 3.8, 6.7, 2.2, Iris-virginica\n",
      "7.7, 2.6, 6.9, 2.3, Iris-virginica\n",
      "6.0, 2.2, 5.0, 1.5, Iris-virginica\n",
      "6.9, 3.2, 5.7, 2.3, Iris-virginica\n",
      "5.6, 2.8, 4.9, 2.0, Iris-virginica\n",
      "7.7, 2.8, 6.7, 2.0, Iris-virginica\n",
      "6.3, 2.7, 4.9, 1.8, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.1, Iris-virginica\n",
      "7.2, 3.2, 6.0, 1.8, Iris-virginica\n",
      "6.2, 2.8, 4.8, 1.8, Iris-virginica\n",
      "6.1, 3.0, 4.9, 1.8, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.1, Iris-virginica\n",
      "7.2, 3.0, 5.8, 1.6, Iris-virginica\n",
      "7.4, 2.8, 6.1, 1.9, Iris-virginica\n",
      "7.9, 3.8, 6.4, 2.0, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.2, Iris-virginica\n",
      "6.3, 2.8, 5.1, 1.5, Iris-virginica\n",
      "6.1, 2.6, 5.6, 1.4, Iris-virginica\n",
      "7.7, 3.0, 6.1, 2.3, Iris-virginica\n",
      "6.3, 3.4, 5.6, 2.4, Iris-virginica\n",
      "6.4, 3.1, 5.5, 1.8, Iris-virginica\n",
      "6.0, 3.0, 4.8, 1.8, Iris-virginica\n",
      "6.9, 3.1, 5.4, 2.1, Iris-virginica\n",
      "6.7, 3.1, 5.6, 2.4, Iris-virginica\n",
      "6.9, 3.1, 5.1, 2.3, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "6.8, 3.2, 5.9, 2.3, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.5, Iris-virginica\n",
      "6.7, 3.0, 5.2, 2.3, Iris-virginica\n",
      "6.3, 2.5, 5.0, 1.9, Iris-virginica\n",
      "6.5, 3.0, 5.2, 2.0, Iris-virginica\n",
      "6.2, 3.4, 5.4, 2.3, Iris-virginica\n",
      "5.9, 3.0, 5.1, 1.8, Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "#loading the data file\n",
    "with open('iris.data.txt', 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        for row in lines:\n",
    "            print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8a5d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 98\n",
      "Test: 51\n"
     ]
    }
   ],
   "source": [
    "#spliting the data into a training dataset \n",
    "\n",
    "\n",
    "def loadDataset(filename, split, trainingSet=[], testSet=[]):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        for x in range(len(dataset) - 1):\n",
    "            # Convert the first 4 columns to floats\n",
    "            for y in range(4):\n",
    "                dataset[x][y] = float(dataset[x][y])\n",
    "            \n",
    "            # Randomly split the data based on the 'split' parameter\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    "                \n",
    "#testing this function\n",
    "trainingSet=[]\n",
    "testSet=[]\n",
    "loadDataset('iris.data.txt', 0.66, trainingSet, testSet)\n",
    "print ('Train: ' + repr(len(trainingSet)))\n",
    "print ('Test: ' + repr(len(testSet)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f053d",
   "metadata": {},
   "source": [
    "2. Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbb367",
   "metadata": {},
   "source": [
    "To make predictions we need to calculate the similarity between any two given data instances. This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn, make a prediction.\n",
    "\n",
    "Given that all four flower measurements are numeric and have the same units, we can directly use the Euclidean distance measure. \n",
    "\n",
    "Additionally, we want to control which fields to include in the distance calculation. Specifically, we only want to include the first 4 attributes. One approach is to limit the Euclidean distance to a fixed length, ignoring the final dimension.\n",
    "\n",
    "Putting all of this together, you have to define the Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fff06df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 3.4641016151377544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    arr1 = np.array(instance1[:length])\n",
    "    arr2 = np.array(instance2[:length])\n",
    "    \n",
    "    if len(arr1) != len(arr2):\n",
    "        raise ValueError()\n",
    "    \n",
    "    return np.linalg.norm(arr1 - arr2)\n",
    "\n",
    "# Example :\n",
    "data1 = [2, 2, 2, 'a']\n",
    "data2 = [4, 4, 4, 'b']\n",
    "distance = euclideanDistance(data1, data2, 3)\n",
    "print('Distance:', repr(distance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deddc17",
   "metadata": {},
   "source": [
    "3. Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b482a",
   "metadata": {},
   "source": [
    "Now that we have a similarity measure, we can use it to collect the k most similar instances for a given unseen instance.\n",
    "\n",
    "This is a straightforward process of calculating the distance for all instances and selecting a subset with the smallest distance values.\n",
    "\n",
    "Below is the getNeighbors function that returns k most similar neighbors from the training set for a given test instance (using the already defined euclideanDistance function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25672ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4, 4, 'b']]\n"
     ]
    }
   ],
   "source": [
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance) - 1\n",
    "    \n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    neighbors = []\n",
    "    \n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "# Example:\n",
    "trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']]\n",
    "testInstance = [5, 5, 5]\n",
    "k = 1\n",
    "\n",
    "neighbors = getNeighbors(trainSet, testInstance, k)\n",
    "print(neighbors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72658d5d",
   "metadata": {},
   "source": [
    "4. Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438bf76",
   "metadata": {},
   "source": [
    "Once we have located the most similar neighbors for a test instance, the next task is to devise a predicted response based on those neighbors.\n",
    "\n",
    "We can do this by allowing each neighbor to vote for their class attribute, and taking the majority vote as the prediction.\n",
    "\n",
    "Below is a function for getting the majority voted response from a number of neighbors. It assumes the class is the last attribute for each neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9defb290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response is: a\n"
     ]
    }
   ],
   "source": [
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    \n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]  # Get the class label from the last element\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    \n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "# Example :\n",
    "neighbors = [[1, 1, 1, 'a'], [2, 2, 2, 'a'], [3, 3, 3, 'b']]\n",
    "response = getResponse(neighbors)\n",
    "print(\"Response is:\",response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01043523",
   "metadata": {},
   "source": [
    "5. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92209c5d",
   "metadata": {},
   "source": [
    "We have all of the pieces of the KNN algorithm in place. An important remaining concern is how to evaluate the accuracy of predictions.\n",
    "\n",
    "An easy way to evaluate the accuracy of the model is to calculate a ratio of the total correct predictions out of all predictions made, called classification accuracy.\n",
    "\n",
    "Below is the getAccuracy function that sums the total correct predictions and returns the accuracy as a percentage of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de624e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    \n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = (correct / float(len(testSet))) * 100.0\n",
    "    return accuracy\n",
    "\n",
    "# Example:\n",
    "testSet = [[1, 1, 1, 'a'], [2, 2, 2, 'a'], [3, 3, 3, 'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af02bd",
   "metadata": {},
   "source": [
    "6. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1cab40b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beit\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Function to load the Iris dataset and split it into training and testing sets\n",
    "def loadDataset(filename, split):\n",
    "    trainingSet = []\n",
    "    testSet = []\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        \n",
    "        for x in range(len(dataset) - 1):\n",
    "            # Convert the first 4 columns to floats\n",
    "            for y in range(4):\n",
    "                dataset[x][y] = float(dataset[x][y])\n",
    "            \n",
    "            # Randomly split the data based on the 'split' parameter\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    "    \n",
    "    return trainingSet, testSet\n",
    "\n",
    "# Main function for the Iris dataset\n",
    "def main():\n",
    "    \n",
    "    filename = 'iris.data.txt'\n",
    "    split = 0.67 \n",
    "    trainingSet, testSet = loadDataset(filename, split)\n",
    "\n",
    "    k = 3  \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "   \n",
    "    X_train = [row[:-1] for row in trainingSet]\n",
    "    y_train = [row[-1] for row in trainingSet]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    X_test = [row[:-1] for row in testSet]\n",
    "\n",
    "    predictions = knn.predict(X_test)\n",
    "\n",
    "    correct = sum(1 for true, pred in zip([row[-1] for row in testSet], predictions) if true == pred)\n",
    "    accuracy = (correct / len(testSet)) * 100.0\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc855de",
   "metadata": {},
   "source": [
    "7. Another distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e765d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beit\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "#i'm going to use the manhattan distance\n",
    "def main():\n",
    "    filename = 'iris.data.txt'\n",
    "    split = 0.67\n",
    "    k = 3\n",
    "\n",
    "    trainingSet, testSet = loadDataset(filename, split)\n",
    "    \n",
    "    # using the  Manhattan distance\n",
    "    def manhattan_distance(x, y):\n",
    "        return sum(abs(a - b) for a, b in zip(x, y))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=manhattan_distance)\n",
    "    \n",
    "    X_train, y_train = zip(*[(row[:-1], row[-1]) for row in trainingSet])\n",
    "    X_test, y_test = zip(*[(row[:-1], row[-1]) for row in testSet])\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efb41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
